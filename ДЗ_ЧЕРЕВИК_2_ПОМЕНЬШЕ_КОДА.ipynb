{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sashachereshnya-pixel/compling2025/blob/main/%D0%94%D0%97_%D0%A7%D0%95%D0%A0%D0%95%D0%92%D0%98%D0%9A_2_%D0%9F%D0%9E%D0%9C%D0%95%D0%9D%D0%AC%D0%A8%D0%95_%D0%9A%D0%9E%D0%94%D0%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiXdJjBa9sCq"
      },
      "source": [
        "# Домашнее задание: Токенизация текста"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дан список текстов, которые нужно токенизировать разными способами"
      ],
      "metadata": {
        "id": "1xVbvaj_phyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\n",
        "\"The quick brown fox jumps over the lazy dog. It's a beautiful day!\",\n",
        "\"Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\",\n",
        "\"I can't believe she's going! Let's meet at Jane's house. They'll love it.\",\n",
        "\"What's the ETA for the package? Please e-mail support@example.com ASAP!\"\n",
        "]"
      ],
      "metadata": {
        "id": "uj-xaNnwpiPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Используйте способы токенизации, которые описаны в практикуме. Каждый способ нужно обернуть в функцию, например:\n",
        "\n",
        " ```python\n",
        " def simple_tokenization(string):\n",
        "   return string.split()\n",
        "   ```"
      ],
      "metadata": {
        "id": "ix1Im4Kcqb3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Напишите функцию для токенизации по пробелам и знакам препинания (используйте оператор `def`)"
      ],
      "metadata": {
        "id": "Ih0BBOGBpv6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ваш код здесь"
      ],
      "metadata": {
        "id": "W1QCaw6cqDnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Напишите функцию для токенизации текста с помощью NLTK"
      ],
      "metadata": {
        "id": "GThvPcovqgO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ваш код здесь"
      ],
      "metadata": {
        "id": "14BIv33iqrkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишите функцию для токенизации текста с помощью Spacy"
      ],
      "metadata": {
        "id": "GxW7ZP6iqwpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ваш код здесь"
      ],
      "metadata": {
        "id": "B0NQg-VfuFW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. С помощью цикла `for` примените каждую из написанных функций к каждому тексту из списка `texts`"
      ],
      "metadata": {
        "id": "WmyJfB9wuKkm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvUmk94MhrL8"
      },
      "outputs": [],
      "source": [
        "# Ваш код здесь"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "\n",
        "# Загрузка необходимых ресурсов NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')  # Добавляем недостающий ресурс\n",
        "\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"Модель spaCy не найдена. Для установки выполните: python -m spacy download en_core_web_sm\")\n",
        "    nlp = None\n",
        "\n",
        "texts = [\n",
        "    \"The quick brown fox jumps over the lazy dog. It's a beautiful day!\",\n",
        "    \"Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\",\n",
        "    \"I can't believe she's going! Let's meet at Jane's house. They'll love it.\",\n",
        "    \"What's the ETA for the package? Please e-mail support@example.com ASAP!\"\n",
        "]\n",
        "\n",
        "def simple_tokenization(text):\n",
        "    \"\"\"\n",
        "    Простая токенизация по пробелам и знакам препинания\n",
        "    Использует регулярные выражения для разделения на слова и пунктуацию\n",
        "    \"\"\"\n",
        "    return re.findall(r\"\\w+|[^\\w\\s]\", text)\n",
        "\n",
        "def nltk_tokenization(text):\n",
        "    \"\"\"\n",
        "    Токенизация с помощью NLTK\n",
        "    Обрабатывает сокращения, пунктуацию и специальные случаи\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return nltk.word_tokenize(text)\n",
        "    except LookupError:\n",
        "        # Альтернатива если punkt_tab недоступен\n",
        "        from nltk.tokenize import TreebankWordTokenizer\n",
        "        tokenizer = TreebankWordTokenizer()\n",
        "        return tokenizer.tokenize(text)\n",
        "\n",
        "def spacy_tokenization(text):\n",
        "    \"\"\"\n",
        "    Токенизация с помощью spaCy\n",
        "    Промышленный токенизатор с лингвистическим анализом\n",
        "    \"\"\"\n",
        "    if nlp is None:\n",
        "        return [\"spaCy модель не доступна\"]\n",
        "    doc = nlp(text)\n",
        "    return [token.text for token in doc]\n",
        "\n",
        "# Применяем все функции к каждому тексту\n",
        "for i, text in enumerate(texts, 1):\n",
        "    print(f\"\\n--- Текст {i} ---\")\n",
        "    print(f\"Оригинал: {text}\")\n",
        "\n",
        "    print(f\"Simple: {simple_tokenization(text)}\")\n",
        "    print(f\"NLTK: {nltk_tokenization(text)}\")\n",
        "    print(f\"spaCy: {spacy_tokenization(text)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BpBMEgyLpqi",
        "outputId": "d7ccf510-e47a-4bbd-bab2-0ebdb735d2be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Текст 1 ---\n",
            "Оригинал: The quick brown fox jumps over the lazy dog. It's a beautiful day!\n",
            "Simple: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'\", 's', 'a', 'beautiful', 'day', '!']\n",
            "NLTK: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day', '!']\n",
            "spaCy: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day', '!']\n",
            "\n",
            "--- Текст 2 ---\n",
            "Оригинал: Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\n",
            "Simple: ['Dr', '.', 'Smith', 'arrived', 'at', '5', ':', '30', 'p', '.', 'm', '.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1', ',', '000', '.', '50', '.']\n",
            "NLTK: ['Dr.', 'Smith', 'arrived', 'at', '5:30', 'p.m.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1,000.50', '.']\n",
            "spaCy: ['Dr.', 'Smith', 'arrived', 'at', '5:30', 'p.m.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1,000.50', '.']\n",
            "\n",
            "--- Текст 3 ---\n",
            "Оригинал: I can't believe she's going! Let's meet at Jane's house. They'll love it.\n",
            "Simple: ['I', 'can', \"'\", 't', 'believe', 'she', \"'\", 's', 'going', '!', 'Let', \"'\", 's', 'meet', 'at', 'Jane', \"'\", 's', 'house', '.', 'They', \"'\", 'll', 'love', 'it', '.']\n",
            "NLTK: ['I', 'ca', \"n't\", 'believe', 'she', \"'s\", 'going', '!', 'Let', \"'s\", 'meet', 'at', 'Jane', \"'s\", 'house', '.', 'They', \"'ll\", 'love', 'it', '.']\n",
            "spaCy: ['I', 'ca', \"n't\", 'believe', 'she', \"'s\", 'going', '!', 'Let', \"'s\", 'meet', 'at', 'Jane', \"'s\", 'house', '.', 'They', \"'ll\", 'love', 'it', '.']\n",
            "\n",
            "--- Текст 4 ---\n",
            "Оригинал: What's the ETA for the package? Please e-mail support@example.com ASAP!\n",
            "Simple: ['What', \"'\", 's', 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e', '-', 'mail', 'support', '@', 'example', '.', 'com', 'ASAP', '!']\n",
            "NLTK: ['What', \"'s\", 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e-mail', 'support', '@', 'example.com', 'ASAP', '!']\n",
            "spaCy: ['What', \"'s\", 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e', '-', 'mail', 'support@example.com', 'ASAP', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqAgf6sGhrL8"
      },
      "source": [
        "##### Критерии оценки (макс. балл == 5):\n",
        "\n",
        "- Функциональность (до 4 баллов)): Все методы работают корректно (запускаем код, и он работает)\n",
        "- Качество кода (до 1 балла): Чистый, документированный код с обработкой ошибок (кратко описать, что вы дополнили самостоятельно, например, \"добавлена токенизация `spacy`\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теоретические вопросы (макс. балл == 5; в ведомость выставляется сумма за практику и теорию)\n",
        "\n",
        "Необходимо дать краткие ответы на вопросы по теме \"токенизация\". В сумме длина ответов на вопрос не должна превышать размер вордовской страницы 14 шрифтом."
      ],
      "metadata": {
        "id": "Mwe1Co6MvibX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Почему простое разделение текста по пробелам и знакам препинания часто является недостаточным для современных NLP-задач? Приведите 2-3 конкретных примера, когда деление текста по символам или словам не работает. (2 балла за полный и подробный ответ на вопрос)\n",
        "\n",
        "2. Сколько токенов во фразе \"You shall know a word by the company it keeps\" в модели GPT-5? Как вы получили это значение? (1 балл за правильный ответ и ссылку на ресурс, с помощью которого вы узнали эту информацию)\n",
        "\n",
        "3. Опишите своими словами работу алгоритма BPE (можно форматировать ответ с использованием списков, 2 балла за корректное описание и ясное изложение ответа)"
      ],
      "metadata": {
        "id": "mgE2bQFXv0MG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Простое разделение по пробелам и знакам препинания недостаточно по следующим причинам:\n",
        "\n",
        "Примеры проблемных случаев:\n",
        "\n",
        "Контракции и клитики:\n",
        "\n",
        "\"I can't\" → [\"I\", \"can\", \"'\", \"t\"] (неверно)\n",
        "\n",
        "Правильно: [\"I\", \"ca\", \"n't\"] или [\"I\", \"can\", \"not\"]\n",
        "\n",
        "Составные слова и специальные конструкции:\n",
        "\n",
        "\"state-of-the-art\" → [\"state\", \"-\", \"of\", \"-\", \"the\", \"-\", \"art\"]\n",
        "\n",
        "\"support@example.com\" → разбивается на части вместо сохранения как целого email\n",
        "\n",
        "Морфологически сложные языки:\n",
        "\n",
        "В чешском: \"nejneobhospodařovávatelnějšími\" (одно слово)\n",
        "\n",
        "В немецком: \"Donaudampfschifffahrtsgesellschaftskapitän\"\n",
        "\n",
        "Многозначная пунктуация:\n",
        "\n",
        "\"5:30 p.m.\" → [\"5\", \":\", \"30\", \"p\", \".\", \"m\", \".\"]\n",
        "\n",
        "\"$1,000.50\" → теряет финансовый контекст\n",
        "\n",
        "2. 10 токенов, код ниже в отдельном поле, использовался Open-AI(GPT-5)\n",
        "Ссылка: https://openai.com/zh-Hans-CN/index/introducing-gpt-5/\n",
        "\n",
        "Ответ получен с помощью токенизации через tiktoken:\n",
        "GPT-4 и GPT-5 используют токенизатор cl100k_base (тот же, что и для моделей GPT-4 и GPT-3.5-turbo). Этот токенизатор делит текст на \"токены\", которые не всегда совпадают со словами. Один токен может быть словом, частью слова или даже пробелом.\n",
        "\n",
        "3. Принцип работы BPE:\n",
        "\n",
        "Инициализация:\n",
        "\n",
        "1. Начинаем с символов (байтов) как элементарных токенов\n",
        "\n",
        "2. Создаем начальный словарь из всех уникальных символов\n",
        "\n",
        "Обучение на корпусе:\n",
        "\n",
        "1. Считаем частоты всех пар соседних токенов\n",
        "\n",
        "2. Находим самую частую пару токенов\n",
        "\n",
        "3. Объединяем эту пару в новый токен\n",
        "\n",
        "4. Добавляем новый токен в словарь\n",
        "\n",
        "5. Повторяем процесс заданное количество раз\n",
        "\n",
        "Процесс токенизации:\n",
        "\n",
        "1. Разбиваем текст на символы\n",
        "\n",
        "2. Итеративно объединяем пары токенов согласно обученному словарю\n",
        "\n",
        "3. Останавливаемся, когда нельзя сделать больше объединений\n",
        "\n",
        "Преимущества:\n",
        "\n",
        "1. Эффективно обрабатывает OOV-слова\n",
        "\n",
        "2. Баланс между словными и символьными представлениями\n",
        "\n",
        "3. Подходит для морфологически богатых языков\n",
        "\n",
        "Пример работы алгоримта ниже в отдельном поле"
      ],
      "metadata": {
        "id": "D6M3jh8PLdG0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TikToken**"
      ],
      "metadata": {
        "id": "4go4XFJWc5mR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-4\")  # GPT-5 использует аналогичный\n",
        "tokens = encoding.encode(\"You shall know a word by the company it keeps\")\n",
        "print(len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xod8sbBwbaES",
        "outputId": "7dc7555b-5e5f-4f48-b02a-5fc12e3b7bfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BPE**"
      ],
      "metadata": {
        "id": "743CZY5rc9Kz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Исходный текст: \"low lower lowest\"\n",
        "\n",
        "Частоты пар: 'l'+'o'=2, 'o'+'w'=3, 'w'+' '=1 и т.д.\n",
        "\n",
        "\n",
        "Первое объединение: 'o'+'w' → 'ow'\n",
        "\n",
        "\n",
        "Словарь пополняется: ['l', 'o', 'w', 'e', 'r', ... , 'ow']"
      ],
      "metadata": {
        "id": "iix0zFukdJZZ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}