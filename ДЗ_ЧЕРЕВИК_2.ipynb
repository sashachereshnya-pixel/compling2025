{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sashachereshnya-pixel/compling2025/blob/main/%D0%94%D0%97_%D0%A7%D0%95%D0%A0%D0%95%D0%92%D0%98%D0%9A_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "816fdf03",
      "metadata": {
        "id": "816fdf03"
      },
      "source": [
        "\n",
        "## 6. Практика\n",
        "\n",
        "Возьмём новостной текст, токенизируем разными способами и сравним.  \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "article = 'Natural language processing (NLP) is a field of artificial intelligence\\\n",
        "that gives computers the ability to understand text and spoken words in much the same way human beings can.'\n",
        "\n",
        "\"\"\"\n",
        "NLTK\n",
        "\"\"\"\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk_tokens = word_tokenize(article)\n",
        "print(\"NLTK:\", nltk_tokens)\n",
        "\n",
        "\"\"\"\n",
        "SpaCy\n",
        "\"\"\"\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(article)\n",
        "spacy_tokens = [token.text for token in doc]\n",
        "print(\"spaCy:\", spacy_tokens)\n",
        "\n",
        "\"\"\"\n",
        "Gensim\n",
        "\"\"\"\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "gensim_tokens = simple_preprocess(article)\n",
        "print(\"Gensim:\", gensim_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS-gCknOFQUo",
        "outputId": "07e62b83-98be-4910-9001-d86e334dfb00"
      },
      "id": "LS-gCknOFQUo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK: ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'artificial', 'intelligencethat', 'gives', 'computers', 'the', 'ability', 'to', 'understand', 'text', 'and', 'spoken', 'words', 'in', 'much', 'the', 'same', 'way', 'human', 'beings', 'can', '.']\n",
            "spaCy: ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'artificial', 'intelligencethat', 'gives', 'computers', 'the', 'ability', 'to', 'understand', 'text', 'and', 'spoken', 'words', 'in', 'much', 'the', 'same', 'way', 'human', 'beings', 'can', '.']\n",
            "Gensim: ['natural', 'language', 'processing', 'nlp', 'is', 'field', 'of', 'artificial', 'gives', 'computers', 'the', 'ability', 'to', 'understand', 'text', 'and', 'spoken', 'words', 'in', 'much', 'the', 'same', 'way', 'human', 'beings', 'can']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e575f14e",
      "metadata": {
        "id": "e575f14e"
      },
      "source": [
        "\n",
        "## 7. Задача для самостоятельного разбора  \n",
        "\n",
        "Прочитать статью:  \n",
        "**\"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models\"** (https://arxiv.org/abs/1604.00788)\n",
        "\n",
        "### Напишите ответы на вопросы:\n",
        "1. Что значит Out-of-Vocabulary?  \n",
        "2. Как эту проблемы решили авторы статьи \"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models\"?\n",
        "3. Какие еще типы токенизации мы разбирали на занятии?  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ваш ответ здесь:\n",
        "\n",
        "1. Out-of-Vocabulary (OOV) — это слова, которые не входят в заранее заданный словарь (вокабуляр) модели. В нейронном машинном переводе (NMT) обычно используют ограниченный словарь самых частых слов, а все остальные слова заменяют на специальный токен <unk>. Это приводит к проблемам, так как модель не может обработать неизвестные слова, что ухудшает качество перевода, особенно для редких слов, имен собственных и слов с сложной морфологией.\n",
        "2. Авторы предложили гибридный подход, который сочетает в себе перевод на уровне слов и символов. Их модель в основном работает на уровне слов, но для редких слов использует символьные компоненты. А именно:\n",
        "\n",
        "На стороне источника (source) для редких слов строится представление на основе символов с помощью рекуррентной нейронной сети (RNN), которая обрабатывает символы слова. Это представление используется вместо стандартного embedding для <unk>.\n",
        "\n",
        "На стороне цели (target) когда модель генерирует <unk>, она запускает символьный декодер, который генерирует слово посимвольно, используя контекст от основного декодера.\n",
        "\n",
        "Таким образом, модель никогда не выдает <unk> в окончательном переводе, а вместо этого генерирует правильные слова, даже если они не входили в словарь. Это позволяет достичь открытого словаря (open vocabulary).\n",
        "\n",
        "3. На занятии мы разбирали следующие типы токенизации:\n",
        "\n",
        "Простые методы: разделение по пробелам, использование регулярных выражений.\n",
        "\n",
        "Токенизация с помощью библиотек: NLTK, spaCy, Stanza, MosesTokenizer, gensim, HuggingFace Tokenizers.\n",
        "\n",
        "Посимвольная токенизация.\n",
        "\n",
        "Комбинированные методы (например, послоговая токенизация для японского, гибридные методы, как в данной статье).\n",
        "\n",
        "В статье также упоминается, что их гибридный метод использует токенизацию на уровне слов и символов, что является комбинированным методом."
      ],
      "metadata": {
        "id": "VpPnbGafikMc"
      },
      "id": "VpPnbGafikMc"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C1ISw0bQIlFi"
      },
      "id": "C1ISw0bQIlFi",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}